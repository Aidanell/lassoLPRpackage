% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lassoLPR.R
\name{lassoLPR}
\alias{lassoLPR}
\title{Perform Lasso Local Polynomial Regression}
\usage{
lassoLPR(x, y, h, p = 10, nlambda = 25, numGridPoints = 401L)
}
\arguments{
\item{x}{vector of covariate observations.}

\item{y}{vector of response observations.}

\item{h}{bandwidth parameter. Distance from estimate point where data will be factored into calculation.}

\item{p}{degree of local polynomials.}

\item{nlambda}{number of lambdas tried for each polynomial estimate. More lambdas can significantly increase compute time}
}
\value{
An list containing a numGridPoints by (p+1) matrix where (i,j) is the (j-1)th derivative estimate at grid Point j. This list also contains a vector of lambdas which show how the penalty parameter changed across x.
}
\description{
This function performs Lasso LPR on a given x and y. Lasso LPR uses
the LASSO (Least Absolute Shrinkage Selection Operator) via glmnet with local
polynomial regression in order to automatically shrink small derivative
estimates to 0.
}
\details{
At each grid point, a local polynomial is fit with a penalty parameter lambda,
calculated with cv.glmnet's coordinate descent algorithm.
This returns a regularized estimate of the first function and its p derivatives at that point.
With the appropriate parameters, Lasso LPR will maintain
the function estimate while significantly improving derivative estimates.
Due to the randomness of obtaining optimal lambda via cv.glmnet, lassoLPR
will lowess smooth the lambdas and refit at each grid point. This significantly
reduces "spikes" in the estimates which come from cv.glmnet's nondeterministic optimization.
}
